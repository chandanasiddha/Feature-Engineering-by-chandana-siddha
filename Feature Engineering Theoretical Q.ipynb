{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adca6acc",
   "metadata": {},
   "source": [
    "### Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08cc67",
   "metadata": {},
   "source": [
    "### 1.What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789ad85",
   "metadata": {},
   "source": [
    "A parameter is a variable used in a function, method, or mathematical equation to influence its behavior or output. Think of it as a placeholder that takes a specific value when the function is executed. \n",
    "\n",
    "In mathematics, parameters define the characteristics of equations. For instance, in the linear equation `y = mx + c`, `m` and `c` are parameters that dictate the slope and the y-intercept of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865e60b",
   "metadata": {},
   "source": [
    "### 2. What is correlation?\n",
    "### What does negative correlation mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82dd42",
   "metadata": {},
   "source": [
    "**Correlation** refers to the relationship between two variables and how they change together. It helps measure the strength and direction of their connection. If two variables move in the same direction (when one increases, the other also increases), they have a **positive correlation**. If they move in opposite directions (when one increases, the other decreases), they have a **negative correlation**.\n",
    "\n",
    "A **negative correlation** means that as one variable increases, the other decreases. For example:\n",
    "- **Temperature & Hot Chocolate Sales:** When temperatures rise, people buy less hot chocolate. But when temperatures drop, hot chocolate sales go up.\n",
    "- **Exercise & Body Fat Percentage:** The more consistently someone exercises, the lower their body fat percentage tends to be.\n",
    "- **Screen Time & Sleep Quality:** Higher screen time often leads to worse sleep quality.\n",
    "\n",
    "Correlation does not imply causation, meaning that just because two variables are related doesn’t mean one directly causes the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61cb4b",
   "metadata": {},
   "source": [
    "### 3. Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e9140",
   "metadata": {},
   "source": [
    "**Machine Learning (ML)** is a branch of artificial intelligence (AI) that enables systems to learn from data and make decisions without explicit programming. Instead of following rigid instructions, ML models improve their performance as they process more data, recognizing patterns and making predictions.\n",
    "\n",
    "### **Main Components of Machine Learning**\n",
    "1. **Data** – The foundation of ML. Raw data is collected, cleaned, and processed for training models.\n",
    "2. **Features** – Specific attributes or properties extracted from data to help the model make predictions.\n",
    "3. **Model** – The mathematical structure or algorithm that learns from data and makes predictions.\n",
    "4. **Training** – The process where the model is fed data and adjusts itself to minimize errors.\n",
    "5. **Evaluation** – The model is tested on unseen data to assess its accuracy and effectiveness.\n",
    "6. **Optimization** – Fine-tuning the model to improve performance and prevent errors like overfitting.\n",
    "7. **Deployment** – Applying the trained model to real-world applications, such as recommendation systems or fraud detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da380c18",
   "metadata": {},
   "source": [
    "### 4. How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0774c",
   "metadata": {},
   "source": [
    "**Loss value** is a key metric that determines how well a machine learning model is performing. It measures the difference between the model's predicted outputs and the actual target values. A **low loss value** indicates that the model is making accurate predictions, while a **high loss value** suggests that the model is struggling to fit the data correctly.\n",
    "\n",
    "### **How Loss Value Helps Evaluate Model Performance**\n",
    "1. **Training Progress** – Loss helps track how well the model is learning over time. A steadily decreasing loss during training suggests improvement.\n",
    "2. **Overfitting Detection** – If the loss is very low on training data but high on test data, the model might be overfitting (memorizing the training data instead of learning patterns).\n",
    "3. **Optimization Direction** – Loss functions guide optimization algorithms (like gradient descent) to adjust model parameters effectively.\n",
    "4. **Comparison Between Models** – Different models or hyperparameter settings can be evaluated based on their loss values to select the best-performing one.\n",
    "\n",
    "There are various types of loss functions used in ML depending on the problem type:\n",
    "- **Mean Squared Error (MSE)** for regression problems.\n",
    "- **Cross-Entropy Loss** for classification problems.\n",
    "- **Hinge Loss** for support vector machines (SVMs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9952b",
   "metadata": {},
   "source": [
    "### 5. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8a3dc",
   "metadata": {},
   "source": [
    "Great question! In statistics and data science, variables are classified into **continuous** and **categorical**, based on the type of values they represent.\n",
    "\n",
    "### **Continuous Variables**\n",
    "These are numeric variables that can take an **infinite** number of values within a given range. They represent measurable quantities and can have decimals or fractions.\n",
    "- **Examples**:\n",
    "  - Height (e.g., 165.3 cm)\n",
    "  - Temperature (e.g., 24.8°C)\n",
    "  - Time (e.g., 3.75 hours)\n",
    "\n",
    "Since continuous variables can take a wide range of values, statistical techniques like mean, standard deviation, and regression analysis are used to study them.\n",
    "\n",
    "### **Categorical Variables**\n",
    "These variables represent distinct **groups or categories** and cannot be measured as numbers.\n",
    "- **Examples**:\n",
    "  - Gender (Male, Female, Other)\n",
    "  - Eye Color (Blue, Brown, Green)\n",
    "  - Type of Car (SUV, Sedan, Truck)\n",
    "\n",
    "Categorical variables can be **nominal** (no order, like colors or names) or **ordinal** (ordered, like education level: High School, Bachelor's, Master's, PhD).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b3008",
   "metadata": {},
   "source": [
    "### 6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed698b64",
   "metadata": {},
   "source": [
    "Handling categorical variables is essential in Machine Learning because many algorithms require numerical input. Since categorical variables consist of labels or categories, they must be transformed into a suitable format before being used in ML models. Here are **common techniques** to handle them:\n",
    "\n",
    "#### **1. Encoding Techniques**\n",
    "These methods convert categorical values into numerical representations:\n",
    "- **Label Encoding** – Assigns a unique integer to each category. Suitable for ordinal data (e.g., education level: High School → 0, Bachelor's → 1, Master's → 2, PhD → 3).\n",
    "- **One-Hot Encoding** – Creates separate binary columns for each category. Ideal for nominal categories (e.g., Color: Red → [1,0,0], Blue → [0,1,0], Green → [0,0,1]).\n",
    "- **Ordinal Encoding** – Similar to label encoding but maintains a meaningful order between values.\n",
    "- **Target Encoding** – Replaces categories with the mean of the target variable (useful in classification problems).\n",
    "\n",
    "#### **2. Feature Engineering**\n",
    "- **Grouping Rare Categories** – If some categories have few occurrences, they can be grouped together (e.g., rare job titles into \"Other\").\n",
    "- **Binary Encoding** – Converts categorical values into binary numbers and splits them into separate columns.\n",
    "- **Frequency Encoding** – Assigns values based on the frequency of each category in the dataset.\n",
    "\n",
    "#### **3. Embedding Methods**\n",
    "- **Word Embeddings (for text data)** – Techniques like Word2Vec or TF-IDF encode text categories.\n",
    "- **Entity Embeddings** – Used in deep learning to represent categorical variables meaningfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08129f56",
   "metadata": {},
   "source": [
    "### 7. What do you mean by training and testing a dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda9def",
   "metadata": {},
   "source": [
    "In Machine Learning, **training and testing a dataset** refers to splitting data into two (or more) sets to evaluate how well a model learns and performs.\n",
    "\n",
    "#### **1. Training Dataset**\n",
    "- The **training dataset** is used to **teach** the model.\n",
    "- The model learns patterns and relationships from this data by adjusting its parameters.\n",
    "- Think of it like studying before an exam—this is where learning happens.\n",
    "\n",
    "#### **2. Testing Dataset**\n",
    "- The **testing dataset** is used to evaluate the model’s performance on unseen data.\n",
    "- It helps determine **how well** the model can make predictions.\n",
    "- This step prevents **overfitting**, ensuring the model generalizes well to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e64d7",
   "metadata": {},
   "source": [
    "### 8. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0468ad",
   "metadata": {},
   "source": [
    "`sklearn.preprocessing` is a module in **Scikit-Learn** that provides essential techniques for **data preprocessing** in Machine Learning. It helps prepare raw data for better model performance.\n",
    "\n",
    "### **Key Functions:**\n",
    "- **Scaling & Normalization:** `StandardScaler`, `MinMaxScaler`, `RobustScaler` adjust feature values to a common scale.\n",
    "- **Encoding Categorical Data:** `LabelEncoder`, `OneHotEncoder`, `OrdinalEncoder` convert categories into numerical format.\n",
    "- **Handling Missing Data:** `SimpleImputer`, `KNNImputer` fill missing values efficiently.\n",
    "- **Feature Engineering:** `PolynomialFeatures`, `Binarizer` create new features.\n",
    "- **Sparse Data Scaling:** `MaxAbsScaler` keeps data within a defined range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85573c5a",
   "metadata": {},
   "source": [
    "### 9. What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b0cd4",
   "metadata": {},
   "source": [
    "A **Test Set** is a portion of a dataset that is **reserved for evaluating** a machine learning model **after** it has been trained. It contains **unseen data**, meaning the model has **never learned from it before**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f90d1",
   "metadata": {},
   "source": [
    "### 10. How do we split data for model fitting (training and testing) in Python?\n",
    "### How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b8ef0",
   "metadata": {},
   "source": [
    "#### **Splitting Data for Model Training & Testing**  \n",
    "In **Machine Learning**, data is split into two sets:\n",
    "1. **Training Set** – Used to train the model by identifying patterns.\n",
    "2. **Test Set** – Used to evaluate model accuracy on unseen data.\n",
    "\n",
    "#### **Approach to Machine Learning Problems**  \n",
    "1. **Define the Problem** – Identify the goal (classification, regression, etc.).\n",
    "2. **Collect & Clean Data** – Handle missing values, outliers, and categorical variables.\n",
    "3. **Exploratory Data Analysis (EDA)** – Analyze trends, correlations, and distributions.\n",
    "4. **Feature Engineering** – Select/create useful features.\n",
    "5. **Split Data** – Divide into training and testing sets.\n",
    "6. **Train Model** – Apply ML algorithms and adjust parameters.\n",
    "7. **Evaluate Performance** – Use accuracy, precision, recall, F1-score.\n",
    "8. **Optimize Model** – Fine-tune hyperparameters.\n",
    "9. **Deploy Model** – Integrate into applications.\n",
    "10. **Monitor & Improve** – Update with new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f4f7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: (8, 2)\n",
      "Testing Set Size: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "#In **Python**, we use `train_test_split()` from Scikit-Learn:\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Feature2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "    'Target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "# Splitting into features (X) and target (y)\n",
    "X = data[['Feature1', 'Feature2']]\n",
    "y = data['Target']\n",
    "\n",
    "# Splitting data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Set Size:\", X_train.shape)\n",
    "print(\"Testing Set Size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54754a75",
   "metadata": {},
   "source": [
    "### 11. Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2011bd",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is a crucial step in Machine Learning because it helps **understand** the data before using it to train a model. Skipping EDA can lead to poor model performance, misleading predictions, or errors in the analysis.\n",
    "\n",
    "#### **Key Reasons to Perform EDA Before Model Fitting:**  \n",
    "1. **Detects Missing Values & Outliers** – Identifies gaps or extreme values that can affect model accuracy.\n",
    "2. **Understands Data Distribution** – Helps visualize feature distributions (normal, skewed, etc.).\n",
    "3. **Identifies Correlations Between Variables** – Determines relationships that may affect feature selection.\n",
    "4. **Feature Engineering & Selection** – Improves dataset quality by selecting important features.\n",
    "5. **Ensures Data Quality** – Cleans inconsistencies or noise that may negatively impact training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b5bb9",
   "metadata": {},
   "source": [
    "### 12. What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e1bf8",
   "metadata": {},
   "source": [
    "**Correlation** is a statistical measure that describes the relationship between two variables and how they change together. It helps determine whether an increase or decrease in one variable is associated with an increase or decrease in another.\n",
    "\n",
    "#### **Types of Correlation:**\n",
    "1. **Positive Correlation** – Both variables move in the **same** direction.  \n",
    "   - Example: More study hours → Higher exam scores.\n",
    "   \n",
    "2. **Negative Correlation** – One variable increases while the other decreases.  \n",
    "   - Example: More exercise → Lower body fat percentage.\n",
    "\n",
    "3. **Zero Correlation** – No relationship between the two variables.  \n",
    "   - Example: Shoe size and intelligence.\n",
    "\n",
    "Correlation is often represented by the **correlation coefficient (r)**, ranging from **-1 to +1**, where:\n",
    "- **+1** indicates a perfect positive correlation.\n",
    "- **-1** indicates a perfect negative correlation.\n",
    "- **0** means no correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b24d9f",
   "metadata": {},
   "source": [
    "### 13. What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4fe2ef",
   "metadata": {},
   "source": [
    "A **negative correlation** means that as one variable increases, the other decreases. It indicates an **inverse relationship** between two factors. \n",
    "\n",
    "#### **Examples of Negative Correlation:**  \n",
    "- **Temperature & Hot Chocolate Sales:** As temperatures rise, hot chocolate sales drop.  \n",
    "- **Exercise & Body Fat Percentage:** More exercise is linked to lower body fat.  \n",
    "- **Screen Time & Sleep Quality:** Increased screen time often leads to decreased sleep quality.  \n",
    "\n",
    "Negative correlation is represented by a **correlation coefficient (r)** between **-1 and 0**, where:\n",
    "- **r = -1** → Perfect negative correlation (variables move in opposite directions).\n",
    "- **r = 0** → No correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227af0f",
   "metadata": {},
   "source": [
    "### 14. How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054df0b3",
   "metadata": {},
   "source": [
    "Correlation measures the relationship between two variables and is useful in **data analysis** and **Machine Learning**.\n",
    "\n",
    "#### **Methods to Compute Correlation:**  \n",
    "1. **Pandas (`.corr()`)** – Computes a correlation matrix for all numerical columns in a dataset.  \n",
    "2. **NumPy (`np.corrcoef()`)** – Calculates correlation between two specific arrays.  \n",
    "3. **SciPy (`spearmanr`, `pearsonr`)** – Provides statistical correlation tests.  \n",
    "4. **Seaborn (`sns.heatmap()`)** – Visualizes correlation matrices for easier interpretation.\n",
    "\n",
    "#### **Interpretation of Correlation Coefficient (r):**  \n",
    "- **r = +1** → Perfect positive correlation  \n",
    "- **r = -1** → Perfect negative correlation  \n",
    "- **r = 0** → No correlation  \n",
    "\n",
    "Correlation analysis helps in feature selection, model accuracy improvement, and understanding dependencies in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4380f0",
   "metadata": {},
   "source": [
    "### 15. What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640466a",
   "metadata": {},
   "source": [
    "#### **What is Causation?**  \n",
    "**Causation** (or **causality**) refers to a relationship where one variable **directly influences** another. If **A causes B**, then a change in A will lead to a predictable change in B.\n",
    "\n",
    "### **Difference Between Correlation and Causation**  \n",
    "- **Correlation** means two variables are related, but it does **not** prove that one causes the other.\n",
    " Example: Ice cream sales increase when temperature rises.  \n",
    "\n",
    "- **Causation** means one variable **directly affects** another, proving a cause-and-effect relationship.\n",
    " Example: Hot weather causes people to buy more ice cream.\n",
    "\n",
    "Although ice cream sales and temperature are correlated, buying ice cream does **not** cause hot weather!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4ea3d",
   "metadata": {},
   "source": [
    "### 16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d9468",
   "metadata": {},
   "source": [
    "### **What is an Optimizer?**  \n",
    "An **optimizer** is an algorithm that helps a Machine Learning model **minimize the loss function** and improve accuracy by adjusting the model’s parameters (weights). It plays a crucial role in deep learning and ensures the model learns effectively during training.\n",
    "\n",
    "### **Types of Optimizers and Examples**  \n",
    "\n",
    "#### **1. Gradient Descent**   \n",
    "- **Description:** Adjusts model weights using gradients to minimize error.  \n",
    "- **Example:** In a linear regression model, gradient descent updates weight `w` to minimize loss:  \n",
    "  \\[\n",
    "  w = w - \\alpha \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "  \\]\n",
    "\n",
    "#### **2. Stochastic Gradient Descent (SGD)**  \n",
    "- **Description:** Updates weights using a **single data point** at a time, making training faster but noisier.  \n",
    "- **Example:** Used in **online learning** where data arrives sequentially, like **real-time stock prediction**.\n",
    "\n",
    "#### **3. Mini-Batch Gradient Descent**\n",
    "- **Description:** A mix between **batch** and **stochastic** gradient descent, updating weights using small batches of data.  \n",
    "- **Example:** Used in **image classification**, where batches of images are processed to improve learning speed.\n",
    "\n",
    "#### **4. Adam (Adaptive Moment Estimation)**   \n",
    "- **Description:** Combines **momentum** and **adaptive learning rates**, making training faster and stable.  \n",
    "- **Example:** Used in **deep learning models**, like **CNNs for image recognition**.\n",
    "\n",
    "#### **5. RMSprop (Root Mean Square Propagation)**   \n",
    "- **Description:** Uses a moving average of squared gradients to adjust learning rates dynamically.  \n",
    "- **Example:** Works well in **recurrent neural networks (RNNs)** for **speech and text analysis**.\n",
    "\n",
    "#### **6. Momentum-Based Optimization** \n",
    "- **Description:** Adds momentum to weight updates to avoid oscillations in learning.  \n",
    "- **Example:** Helps in **training deep networks faster**, like **speech-to-text models**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1dd334",
   "metadata": {},
   "source": [
    "### 17. What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a087b",
   "metadata": {},
   "source": [
    "`sklearn.linear_model` is a module in **Scikit-Learn** that provides various **linear models** used for **regression** and **classification** tasks in Machine Learning. These models assume that the relationship between input features and the target variable follows a **linear equation**.\n",
    "\n",
    "#### **Key Models in `sklearn.linear_model`:**  \n",
    "1. **Linear Regression** – Used for predicting continuous values based on a straight-line relationship.  \n",
    "2. **Logistic Regression** – Used for classification problems (binary or multi-class).  \n",
    "3. **Ridge & Lasso Regression** – Regularized linear models to prevent overfitting.  \n",
    "4. **Stochastic Gradient Descent (SGD)** – A scalable optimization method for large datasets.  \n",
    "5. **Perceptron** – A simple linear classifier based on decision boundaries.  \n",
    "\n",
    "Linear models are widely used due to their simplicity, interpretability, and efficiency, making them suitable for a variety of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa5a3e",
   "metadata": {},
   "source": [
    "### 18. What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69122458",
   "metadata": {},
   "source": [
    "`model.fit()` is a method in **Scikit-Learn** and other ML libraries used to **train a model** by learning patterns from the given data. It adjusts the model's parameters based on the training data to minimize errors and improve predictions.\n",
    "\n",
    "#### **Arguments Required for `model.fit()`**  \n",
    "1. **Training Data (`X`)** – The feature matrix containing input variables.  \n",
    "2. **Target Labels (`y`)** – The output labels corresponding to `X`.  \n",
    "\n",
    "Optional arguments:  \n",
    "- **Epochs (`epochs`)** – Number of training cycles (in deep learning models).  \n",
    "- **Batch Size (`batch_size`)** – Defines how many samples are processed at once.  \n",
    "- **Validation Data (`validation_data`)** – Used for evaluating model performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4357878",
   "metadata": {},
   "source": [
    "### 19. What does model.predict() do? What arguments must be given?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e0490",
   "metadata": {},
   "source": [
    "`model.predict()` is a method in **Scikit-Learn** used to make predictions on **new, unseen data** after a model has been trained. It applies the learned patterns (from `model.fit()`) to input data and returns predicted values.\n",
    "\n",
    "#### **Arguments Required for `model.predict()`**  \n",
    "1. **Input Data (`X`)** – The feature matrix for which predictions are needed.  \n",
    "2. **Shape Compatibility** – The structure of `X` should match the format of training data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99040874",
   "metadata": {},
   "source": [
    "### 20. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112ffac",
   "metadata": {},
   "source": [
    "**Continuous variables** are often used for **numerical predictions** (like regression). **Categorical variables** require **encoding** to be used in ML models (like classification).  \n",
    "\n",
    "#### **1. Continuous Variables**  \n",
    "- Represent **measurable** quantities.  \n",
    "- Can take **infinite** values within a given range.  \n",
    "- Often have **decimals or fractions**.  \n",
    "- Example: **Height (cm), Temperature (°C), Weight (kg), Time (hours)**.  \n",
    "\n",
    "#### **2. Categorical Variables**  \n",
    "- Represent **distinct groups or labels**.  \n",
    "- Cannot be measured as numbers.  \n",
    "- Can be **nominal** (unordered categories) or **ordinal** (ordered categories).  \n",
    "- Example: **Gender (Male, Female, Other), Eye Color (Blue, Brown, Green), Education Level (High School, Bachelor's, Master's, PhD)**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53561da3",
   "metadata": {},
   "source": [
    "### 21. What is feature scaling? How does it help in Machine Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33483bd2",
   "metadata": {},
   "source": [
    "**Feature Scaling** is a preprocessing technique in Machine Learning that transforms numerical data into a standardized range. This ensures that all features contribute equally to model learning, preventing bias toward larger-valued variables.\n",
    "\n",
    "#### **Why is Feature Scaling Important?**  \n",
    "1. **Improves Model Convergence** – Many ML algorithms (like Gradient Descent) perform better when data is scaled.  \n",
    "2. **Prevents Numerical Instability** – Large differences in feature values can lead to inaccurate predictions.  \n",
    "3. **Enhances Distance-Based Models** – Scaling is critical for algorithms like k-NN and SVM, which rely on Euclidean distance.  \n",
    "4. **Optimizes Performance in Neural Networks** – Ensures stable weight updates during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff19937",
   "metadata": {},
   "source": [
    "### 22. How do we perform scaling in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a6227",
   "metadata": {},
   "source": [
    "Feature scaling is an essential preprocessing step in Machine Learning to bring numerical values into a consistent range. In Python, **Scikit-Learn** provides various scaling techniques.\n",
    "\n",
    "#### **Common Scaling Methods:**  \n",
    "1. **Standardization (`StandardScaler`)** – Transforms data to have a **mean of 0** and **standard deviation of 1**.\n",
    "2. **Min-Max Scaling (`MinMaxScaler`)** – Scales values between a specific **range (0 to 1)**.\n",
    "3. **Robust Scaling (`RobustScaler`)** – Uses **median and interquartile range**, effective against outliers.\n",
    "4. **Normalization (`Normalizer`)** – Rescales values **based on vector norms**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8d13d",
   "metadata": {},
   "source": [
    "### 23. What is sklearn.preprocessing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3ed39",
   "metadata": {},
   "source": [
    "`sklearn.preprocessing` is a module in **Scikit-Learn** that provides essential techniques for **data preprocessing** in Machine Learning. It helps transform raw data into a suitable format for better model performance.\n",
    "\n",
    "#### **Key Functions in `sklearn.preprocessing`:**\n",
    "- **Feature Scaling:** `StandardScaler`, `MinMaxScaler`, `RobustScaler` adjust feature values to a common scale.\n",
    "- **Encoding Categorical Data:** `LabelEncoder`, `OneHotEncoder`, `OrdinalEncoder` convert categorical variables into numerical format.\n",
    "- **Handling Missing Data:** `SimpleImputer`, `KNNImputer` fill missing values efficiently.\n",
    "- **Feature Engineering:** `PolynomialFeatures`, `Binarizer` create new features.\n",
    "- **Normalization:** `Normalizer` adjusts feature values based on vector norms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d60802",
   "metadata": {},
   "source": [
    "### 24. How do we split data for model fitting (training and testing) in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37763e",
   "metadata": {},
   "source": [
    "In Machine Learning, splitting data into **training** and **testing** sets ensures that the model learns from one part of the data and is evaluated on another. This prevents **overfitting** and helps measure **generalization performance**.\n",
    "\n",
    "#### **Key Concepts of Data Splitting**\n",
    "1. **Training Set:** Used to train the model, allowing it to learn patterns.\n",
    "2. **Testing Set:** Used to evaluate the model on unseen data.\n",
    "3. **Validation Set (optional):** Helps fine-tune model parameters before final testing.\n",
    "\n",
    "#### **Common Data Splitting Ratios**\n",
    "- **80-20 Split:** 80% for training, 20% for testing (common practice).\n",
    "- **70-30 Split:** 70% for training, 30% for testing.\n",
    "- **60-20-20 Split:** 60% for training, 20% for validation, 20% for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150bbf7",
   "metadata": {},
   "source": [
    "### 25. Explain data encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320d1b5",
   "metadata": {},
   "source": [
    "Data encoding is a technique used in **Machine Learning** to convert **categorical variables** into numerical representations. Since ML algorithms require numerical input, encoding ensures models can process categorical data efficiently.\n",
    "\n",
    "#### **Types of Data Encoding:**  \n",
    "1. **Label Encoding** – Assigns **integer values** to each category (e.g., Red → 0, Blue → 1, Green → 2).  \n",
    "2. **One-Hot Encoding** – Creates separate **binary columns** for each category (e.g., Red → [1,0,0], Blue → [0,1,0], Green → [0,0,1]).  \n",
    "3. **Ordinal Encoding** – Assigns ordered **numeric values** to categories with a meaningful sequence (e.g., Education Level: High School → 0, Bachelor's → 1, Master's → 2).  \n",
    "4. **Target Encoding** – Replaces categories with **mean target values** from the dataset, useful in classification tasks.  \n",
    "5. **Binary Encoding** – Converts categories into **binary numbers** to reduce dimensionality.  \n",
    "6. **Frequency Encoding** – Assigns values based on **how often** each category appears in the dataset.\n",
    "\n",
    "#### **Why is Encoding Important?**  \n",
    "- Ensures ML models can interpret categorical data properly.  \n",
    "- Helps improve accuracy and prevent bias.  \n",
    "- Optimizes computational efficiency in learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a0cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
